# ü´Ä Heart Disease Prediction - Multi-Language Project

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![R](https://img.shields.io/badge/R-4.0%2B-blue)
![Julia](https://img.shields.io/badge/Julia-1.6%2B-purple)
![Status](https://img.shields.io/badge/Status-Active-success)
![Kaggle](https://img.shields.io/badge/Kaggle-Competition-orange)

## üìã Contexte du Projet

Ce projet s'inscrit dans le cadre de la comp√©tition Kaggle **[Playground Series S6E2: Predicting Heart Disease](https://www.kaggle.com/competitions/playground-series-s6e2/data)**.

L'objectif est de d√©velopper un mod√®le de Machine Learning robuste pour pr√©dire la pr√©sence de maladies cardiaques chez des patients, en se basant sur des indicateurs cliniques (√¢ge, cholest√©rol, r√©sultats ECG, etc.).

**Particularit√© du projet :** Ce d√©p√¥t d√©montre une approche **polyglotte** de la Data Science, tirant parti des forces de chaque langage :
* **R** : Analyse Exploratoire (EDA) et Feature Engineering statistique.
* **Julia** : Nettoyage haute performance des donn√©es brutes.
* **Python** : Entra√Ænement des mod√®les (XGBoost), Pipeline ML et MLOps.

---

## üèóÔ∏è Architecture du Projet

La structure du projet s√©pare clairement les donn√©es, les notebooks d'exp√©rimentation et le code source modulaire.

```text
Base/
‚îÇ
‚îú‚îÄ‚îÄ README.md                 # Documentation principale du projet
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances Python (pandas, xgboost, shap...)
‚îú‚îÄ‚îÄ .gitignore                # Exclusion des fichiers lourds et temporaires
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Donn√©es brutes (train.csv, test.csv)
‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Donn√©es nettoy√©es pr√™tes pour le ML (train_processed.csv)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                # Espace d'exp√©rimentation par langage
‚îÇ   ‚îú‚îÄ‚îÄ Julia/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nettoyage.jl             # Script de nettoyage haute performance
‚îÇ   ‚îú‚îÄ‚îÄ Python/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_eda_exploratory.ipynb # EDA comparative
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_model_training.ipynb  # Pipeline, GridSearch & XGBoost
‚îÇ   ‚îî‚îÄ‚îÄ R/
‚îÇ       ‚îú‚îÄ‚îÄ 01_eda_exploratory.R     # Analyse statistique approfondie
‚îÇ       ‚îî‚îÄ‚îÄ 02_feature_engineering.R # Cr√©ation de variables et imputation
‚îÇ
‚îú‚îÄ‚îÄ src/                      # Code source Python (Scripts de production)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py      # Pipelines de transformation sklearn
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py      # Fonctions graphiques personnalis√©es
‚îÇ
‚îî‚îÄ‚îÄ models/                   # Artefacts des mod√®les entra√Æn√©s (.joblib, .pkl)

## üîÑ Pipeline Data Science

Le flux de travail suit une m√©thodologie rigoureuse, allant du nettoyage haute performance √† l'inf√©rence, en garantissant la reproductibilit√©.

```mermaid
graph LR
A[Collecte Data] --> B[Nettoyage & EDA]
B --> C[Feature Engineering]
C --> D[Entra√Ænement Mod√®le]
D --> E[Optimisation Seuil]
E --> F[Pr√©diction Finale]


1. Collecte : Importation des donn√©es brutes de la comp√©tition Kaggle.

2. Nettoyage & EDA (R/Julia) :
    - Identification des valeurs aberrantes et imputation intelligente.
    - Typage strict des variables (cat√©gorielles vs num√©riques).

3. Feature Engineering (R) :
    - Cr√©ation de ratios m√©dicaux (ex: Risk Ratio).
    - Transformation log des variables asym√©triques (st_depression).
    - Encodage One-Hot des variables cat√©gorielles.

4. Mod√©lisation (Python) :
    - Utilisation de XGBoost avec acc√©l√©ration histogramme (tree_method='hist').
    - Optimisation des hyperparam√®tres via RandomizedSearchCV pour g√©rer la volum√©trie (600k+ lignes).
    - Strat√©gie de validation crois√©e (StratifiedKFold).

5. Optimisation : Ajustement du seuil de d√©cision (Threshold Tuning) pour maximiser le F1-Score et la sensibilit√© m√©dicale.


## üõ†Ô∏è Stack Technique

Langages : Python, R, Julia.

Libraries Python : scikit-learn, xgboost, pandas, seaborn, joblib.

Libraries R : tidyverse, caret, janitor.

Mod√©lisation : Pipeline Scikit-Learn, Gradient Boosting, Hyperparameter Tuning.

## üöÄ Comment lancer le projet

Installation :
Cloner le d√©p√¥t et installer les d√©pendances Python :

git clone [https://github.com/votre-username/heart-disease-prediction.git](https://github.com/votre-username/heart-disease-prediction.git)
cd heart-disease-prediction
pip install -r requirements.txt

3. Entra√Ænement du mod√®le
Lancer le script d'entra√Ænement optimis√© pour g√©n√©rer le mod√®le final et les pr√©dictions :

via le notebook : notebooks/Python/03_model_training.ipynb

## üìä R√©sultats

Mod√®le : XGBoost Classifier (Optimis√©)
Strat√©gie : Randomized Search sur 15 it√©rations + Threshold Tuning.
M√©trique Cible : F1-Score (√âquilibre Pr√©cision/Rappel).
Output : Fichier submission.csv pr√™t pour Kaggle.